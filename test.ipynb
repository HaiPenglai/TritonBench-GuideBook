{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20abc62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "操作系统: Linux-5.15.0-94-generic-x86_64-with-glibc2.35\n",
      "Python 版本: 3.12.3\n",
      "PyTorch 版本: 2.8.0+cu128\n",
      "Triton 版本: 3.4.0\n",
      "\n",
      "✅ CUDA 已就位!\n",
      "显卡型号: NVIDIA GeForce RTX 5090\n",
      "显卡算力: (12, 0)\n",
      "当前显存占用: 0.00 MB\n",
      "硬件是否支持 FP8: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import platform\n",
    "\n",
    "print(f\"操作系统: {platform.platform()}\")\n",
    "print(f\"Python 版本: {platform.python_version()}\")\n",
    "print(f\"PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"Triton 版本: {triton.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n✅ CUDA 已就位!\")\n",
    "    print(f\"显卡型号: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"显卡算力: {torch.cuda.get_device_capability(0)}\")\n",
    "    print(f\"当前显存占用: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # 5090 专属：验证 FP8 支持 (Blackwell 核心特性)\n",
    "    print(f\"硬件是否支持 FP8: {torch.cuda.get_device_capability(0) >= (10, 0)}\")\n",
    "else:\n",
    "    print(f\"❌ 警告: CUDA 未识别，请检查驱动！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d43a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 正在调用 deepseek/deepseek-v3.2 生成 Triton 算子 ---\n",
      "\n",
      "AI 实时生成中:\n",
      "------------------------------\n",
      "```python\n",
      "import torch\n",
      "import triton\n",
      "import triton.language as tl\n",
      "\n",
      "\n",
      "@triton.jit\n",
      "def add_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n",
      "    pid = tl.program_id(axis=0)\n",
      "    block_start = pid * BLOCK_SIZE\n",
      "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
      "    mask = offsets < n_elements\n",
      "    \n",
      "    x = tl.load(in_ptr0 + offsets, mask=mask)\n",
      "    y = tl.load(in_ptr1 + offsets, mask=mask)\n",
      "    output = x + y\n",
      "    tl.store(out_ptr + offsets, output, mask=mask)\n",
      "\n",
      "\n",
      "def add_wrapper(x, y):\n",
      "    assert x.is_cuda and y.is_cuda\n",
      "    assert x.dtype == y.dtype\n",
      "    assert x.shape == y.shape\n",
      "    \n",
      "    n_elements = x.numel()\n",
      "    out = torch.empty_like(x)\n",
      "    \n",
      "    if n_elements == 0:\n",
      "        return out\n",
      "    \n",
      "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
      "    \n",
      "    BLOCK_SIZE = triton.next_power_of_2(min(1024, n_elements))\n",
      "    \n",
      "    add_kernel[grid](x, y, out, n_elements, BLOCK_SIZE=BLOCK_SIZE)\n",
      "    \n",
      "    return out\n",
      "```\n",
      "------------------------------\n",
      "生成完毕！正在格式化输出...\n",
      "\n",
      "==== 一键粘贴到 JSON 'predict' 字段的内容 ====\n",
      "\"import torch\\nimport triton\\nimport triton.language as tl\\n\\n\\n@triton.jit\\ndef add_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\\n    pid = tl.program_id(axis=0)\\n    block_start = pid * BLOCK_SIZE\\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\\n    mask = offsets < n_elements\\n    \\n    x = tl.load(in_ptr0 + offsets, mask=mask)\\n    y = tl.load(in_ptr1 + offsets, mask=mask)\\n    output = x + y\\n    tl.store(out_ptr + offsets, output, mask=mask)\\n\\n\\ndef add_wrapper(x, y):\\n    assert x.is_cuda and y.is_cuda\\n    assert x.dtype == y.dtype\\n    assert x.shape == y.shape\\n    \\n    n_elements = x.numel()\\n    out = torch.empty_like(x)\\n    \\n    if n_elements == 0:\\n        return out\\n    \\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\\n    \\n    BLOCK_SIZE = triton.next_power_of_2(min(1024, n_elements))\\n    \\n    add_kernel[grid](x, y, out, n_elements, BLOCK_SIZE=BLOCK_SIZE)\\n    \\n    return out\"\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def test_openrouter_triton_demo():\n",
    "    # 1. 配置参数[提示：这个API key仅在视频展示中有效]\n",
    "    api_key = \"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\"\n",
    "    model_name = \"deepseek/deepseek-v3.2\" # 也可以换成 deepseek/deepseek-r1 获取更强逻辑\n",
    "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "    # 【优化：强制 AI 闭嘴的提示词】\n",
    "    # 我们加入 System 角色，并要求只输出代码块，不解释\n",
    "    user_content = \"\"\"\n",
    "    Write a Triton kernel for element-wise addition as described: \n",
    "    - Kernel: `add_kernel(in_ptr0, in_ptr1, out_ptr, n_elements, BLOCK_SIZE: tl.constexpr)`\n",
    "    - Wrapper: `add_wrapper(x, y)`\n",
    "    \n",
    "    IMPORTANT: Output ONLY the code within a single ```python code block. \n",
    "    Do not provide any introductory text or explanation. \n",
    "    Do not say \"Here is the code\". Just start with the code.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional Triton GPU programmer. Output only code, no conversational text.\"},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ],\n",
    "        \"stream\": True \n",
    "    }\n",
    "\n",
    "    print(f\"--- 正在调用 {model_name} 生成 Triton 算子 ---\\n\")\n",
    "\n",
    "    full_response = \"\"\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(payload), stream=True)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"请求失败: {response.text}\")\n",
    "            return\n",
    "\n",
    "        print(\"AI 实时生成中:\\n\" + \"-\"*30)\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                line_str = line.decode('utf-8')\n",
    "                if line_str.startswith(\"data: \"):\n",
    "                    data_content = line_str[6:]\n",
    "                    if data_content.strip() == \"[DONE]\": break\n",
    "                    \n",
    "                    try:\n",
    "                        chunk_json = json.loads(data_content)\n",
    "                        content = chunk_json['choices'][0].get('delta', {}).get('content', '')\n",
    "                        if content:\n",
    "                            full_response += content\n",
    "                            sys.stdout.write(content)\n",
    "                            sys.stdout.flush()\n",
    "                    except: continue\n",
    "\n",
    "        print(\"\\n\" + \"-\"*30 + \"\\n生成完毕！正在格式化输出...\\n\")\n",
    "\n",
    "        # 【后处理：提取代码并格式化为 JSON 字符串】\n",
    "        # 提取 ```python ... ``` 之间的内容\n",
    "        code_match = re.search(r\"```python\\s+(.*?)\\s+```\", full_response, re.DOTALL)\n",
    "        if code_match:\n",
    "            raw_code = code_match.group(1).strip()\n",
    "        else:\n",
    "            raw_code = full_response.strip()\n",
    "\n",
    "        # 生成可以直接粘贴进 JSON 的转义字符串（包含 \\n 和 \\t）\n",
    "        json_ready_string = json.dumps(raw_code)\n",
    "\n",
    "        print(\"==== 一键粘贴到 JSON 'predict' 字段的内容 ====\")\n",
    "        print(json_ready_string)\n",
    "        print(\"============================================\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"发生错误: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_openrouter_triton_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
